{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewlee-29/Applied_Deep_Learning_2023Summer/blob/main/Week%202%3A%20Intro_to_NN_with_Keras_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q4Ib19cxm4k"
      },
      "source": [
        "# Intro to NN with Keras\n",
        "\n",
        "- Design our fist neural net to solve a regression problem\n",
        "- Introduce tensorflow's Keras library\n",
        "- Evaluating trained model\n",
        "- Visualizing the loss\n",
        "- Improvements:\n",
        "  - Hidden layers\n",
        "  - Pre-processing: Scaling\n",
        "  - Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpohwQHhxm4m"
      },
      "source": [
        "### Review of SciKit API\n",
        "\n",
        "According to VanderPlas (2016):\n",
        "1. Choose a class of model by **importing the appropriate estimator** class from Scikit-Learn.\n",
        "2. Choose model **hyperparameters** by instantiating this class with desired values.\n",
        "3. Arrange data into a **features matrix and target vector** following the discussion above.\n",
        "4. **Fit the model** to your data by calling the ``fit()`` method of the model instance.\n",
        "5. **Apply the model** to new data:\n",
        "   - For supervised learning, often we predict labels for unknown data using the ``predict()`` method.\n",
        "   - For unsupervised learning, we often transform or infer properties of the data using the ``transform()`` or ``predict()`` method. (p. 347)\n",
        "   \n",
        "   \n",
        "``` python\n",
        "from sklearn.svm import SVC #1\n",
        "model = SVC(kernel='linear') #2\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(data.features, data.labels) #3a/b\n",
        "\n",
        "model.fit(Xtrain, ytrain) #4\n",
        "\n",
        "y_model = model.predict(Xtest) #5\n",
        "```\n",
        "\n",
        "VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. Sebastopol, CA: O'Reilly Media."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0dxH2DAxm4n"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "Hypothesis\n",
        "\n",
        "$h(x) = \\sum_{i=1}^n wx_i $\n",
        "\n",
        "Prediction art\n",
        "\n",
        "$x_0 \\rightarrow w_0 \\searrow $\n",
        "\n",
        "$x_1 \\rightarrow w_1 \\rightarrow $   $\\sum \\longrightarrow $ h(x)\n",
        "\n",
        "$\\ldots$\n",
        "\n",
        "$x_m \\rightarrow w_m \\nearrow $\n",
        "\n",
        "[Cost function](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnNjktvNxm4n"
      },
      "source": [
        "## Classification via Logistic Regression\n",
        "\n",
        "Hypothesis\n",
        "\n",
        "Let $y = \\{-1, 1 \\}$ or $y = \\{0, 1 \\}$\n",
        "\n",
        "\n",
        "$ h(x) = \\text{sigmoid}(\\sum_{i=1}^n wx_i ) $\n",
        "\n",
        "Prediction art\n",
        "\n",
        "$x_0 \\rightarrow w_0 \\searrow $\n",
        "\n",
        "$x_1 \\rightarrow w_1 \\rightarrow $   $\\sum  \\rightarrow$ Sigmoid() $\\longrightarrow $ h(x)\n",
        "\n",
        "$\\ldots$\n",
        "\n",
        "$x_m \\rightarrow w_m \\nearrow $\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp0GniiDxm4n"
      },
      "source": [
        "# Regression: Housing Cost Prediciton\n",
        "\n",
        "- Linear Regression approach via Scikit\n",
        "- NN via tf.Keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BOS discontinued. need earlier scikit\n",
        "!pip install scikit-learn==1.1.3\n",
        "#restart runtime when asked"
      ],
      "metadata": {
        "id": "1_CBqda03ZTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfeTou6gxm4o"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCAo_NLhxm4o"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "#Boston housing prices\n",
        "boston = load_boston()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt02Ws5Cxm4p"
      },
      "outputs": [],
      "source": [
        "#Format: Dictionary\n",
        "print(boston.keys())\n",
        "#Size it up\n",
        "print(boston.data.shape)\n",
        "#What are the 13 features?\n",
        "print(boston.feature_names)\n",
        "\n",
        "boston.target.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9SU0TvzUxm4q"
      },
      "outputs": [],
      "source": [
        "X = boston.data\n",
        "y = boston.target\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMX0-J8Fxm4q"
      },
      "outputs": [],
      "source": [
        "#split into Train / Test data:\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=42)\n",
        "\n",
        "print(Xtrain.shape, ytrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psDx6m07xm4r"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(Xtrain, ytrain)\n",
        "\n",
        "#Note: if error: you may need np.asarray(...,dtype=np.float64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CZy17Kk8xm4r"
      },
      "outputs": [],
      "source": [
        "#Evaluate on test data\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "y_model = model.predict(Xtest)\n",
        "#Calculate error and plot\n",
        "mae = mean_squared_error(ytest, y_model)\n",
        "print(\"MAE: \", mae)\n",
        "\n",
        "#compare labels\n",
        "plt.scatter(ytest, y_model)\n",
        "plt.xlabel(\"Truth\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Boston Housing True vs Predicted prices\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_mMYNG5xm4s"
      },
      "source": [
        "## Regression via NN\n",
        "\n",
        "Will use DENSE layers: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwaPZ4sTxm4s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "#Old Syntax:\n",
        "# from keras import layers\n",
        "\n",
        "#Define your NN arch\n",
        "def myNN(train_data):\n",
        "#Initiate model:\n",
        "\n",
        "\n",
        "#First layer is special: It needs to match the shape of data:\n",
        "    # units = 64: # of neurons; determines output shape of the layer.\n",
        "    # input_shape: how many W needed per neuron?\n",
        "    # Thus: input arrays of shape (*, 13). Output arrays of shape: (*, 64)\n",
        "\n",
        "\n",
        "#Hidden layers will auto-size their input based the previous layer:\n",
        "    #Can add more layers here to make it deeper...\n",
        "\n",
        "\n",
        "#Output: Need a single number as our prediction:\n",
        "    #Note that output layer has no activation\n",
        "    # b/c this is a regression problem, not classification.\n",
        "#How should the weights be calculated?\n",
        "\n",
        "#Cost design: loss function?\n",
        "\n",
        "    return model\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AnxSNIxxm4s"
      },
      "outputs": [],
      "source": [
        "# %load keras-ex1.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1GuvUflxm4s"
      },
      "outputs": [],
      "source": [
        "#build the model\n",
        "model = myNN(Xtrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwUAqzAixm4t"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0yIIlhJxm4t"
      },
      "source": [
        "### Ex:\n",
        "\n",
        "Why does it have 896 parameters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sx5HS0Axm4t"
      },
      "outputs": [],
      "source": [
        "#Algebra goes here?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv7garn9xm4t"
      },
      "source": [
        "## Train/Test your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vyd6b2pxm4u"
      },
      "outputs": [],
      "source": [
        "#Train: a la scikit:\n",
        "#Scikit: model.fit(Xtrain, ytrain)\n",
        "ne = 10\n",
        "model.fit(Xtrain, ytrain, epochs=ne, batch_size=1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWQWXlXgxm4u"
      },
      "outputs": [],
      "source": [
        "#Evalute our model:\n",
        "#SciKit:\n",
        "# y_model = model.predict(Xtest)\n",
        "#Calculate error and plot\n",
        "# mae = mean_squared_error(ytest, y_model)\n",
        "\n",
        "nnmse, nnmae = model.evaluate(Xtest, ytest, verbose = 1)\n",
        "print('NN Test MAE: ', nnmae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8UH6x_Zxm4u"
      },
      "source": [
        "### Keras model training API:\n",
        "\n",
        "model.___()\n",
        "\n",
        "- compile(): Configure model to be trained. Cost/loss function selection\n",
        "- fit(): train for given # of iterations/epochs\n",
        "- evaluate(): calculate loss given metrics\n",
        "- predict(): what is my trained model's hypothesis?\n",
        "\n",
        "More info at https://keras.io/api/models/model_training_apis/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB0v88Qbxm4u"
      },
      "source": [
        "## Ex\n",
        "\n",
        "Giving the feeatures for a new house, what do you think it costs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8ILb2D4xm4u"
      },
      "outputs": [],
      "source": [
        "Xnew = np.array([2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
        "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
        "  9.1400e+00])\n",
        "\n",
        "## Predict ynew from Xnew:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtPgI7EGxm4u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QUEMDL_xm4v"
      },
      "source": [
        "## How can we improve?\n",
        "\n",
        "- tune network: add layers, train longer (i.e. epochs)\n",
        "- cross-validation\n",
        "- feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmzjJSdZxm4v"
      },
      "source": [
        "## Ex:\n",
        "Tune the network above to drop the MAE.\n",
        "\n",
        "Hint: Note that loss keeps dropping. Train the NN for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amBDn204xm4v"
      },
      "outputs": [],
      "source": [
        "#Code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M02eXJTpxm4v"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "What if we scale X?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tKGXqMExm4v"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "print('Pre-scale: ', Xtrain[:2])\n",
        "scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
        "Xtrains = scaler.transform(Xtrain)\n",
        "#Apply the learned scaler to Test data:\n",
        "Xtests = scaler.transform(Xtest)\n",
        "print('Post-scale', Xtrains[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJlLde1Axm4v"
      },
      "outputs": [],
      "source": [
        "# for fun: we know how to do this ourselves:\n",
        "xs = (Xtrain - Xtrain.mean(axis=0)) / (Xtrain.std(axis=0))\n",
        "print('Post-scale', xs[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YBibAm_xm4w"
      },
      "source": [
        "## Ex:\n",
        "How well does the scaled data do in 10 epochs, with 1 hidden layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjNpt1Uxm4w"
      },
      "outputs": [],
      "source": [
        "#Solution goes here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxYPKjzxxm4w"
      },
      "source": [
        "## Overview\n",
        "\n",
        "- We created a MLP to predict housing prices\n",
        "\n",
        "- Exposed to Keras/tf API\n",
        "\n",
        "- Employed Dense hidden layers\n",
        "\n",
        "- Discussed how to improve NN results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr3vSlJFxm4w"
      },
      "source": [
        "## NN Steps\n",
        "- Data prep: Split train/test, pre-process, etc.\n",
        "- Create model:\n",
        "    - `model.Sequential()`\n",
        "    - `model.add(layersâ€¦.)`\n",
        "    - Compile: loss function?\n",
        "- Build model/review model summary: `model = myNN(Xtrains)`\n",
        "- Train the model: `model.fit()`\n",
        "- Evaluate on test data / cross-validate\n",
        "- Repeat all of the steps above until satisfied/ran out of time\n",
        "- Ready to predict on new data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh7kOvC2xm4w"
      },
      "source": [
        "## To Do\n",
        "\n",
        "- What is a Dense layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIy9rhc-xm4w"
      },
      "source": [
        "## Bonus Materials\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ODs2HMxm4w"
      },
      "source": [
        "## Plotting the loss\n",
        "\n",
        "We can save the train/test history and plot it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwyj2uHQxm4x"
      },
      "outputs": [],
      "source": [
        "ne = 20\n",
        "#Combine Train and Test to 1-step:\n",
        "history = model.fit(Xtrain, ytrain, epochs=ne, verbose=1, validation_data = (Xtest, ytest)) #, batch_size=1\n",
        "#Evalute our model:\n",
        "# nnmse, nnmae = model.evaluate(Xtest, ytest, verbose = 1)\n",
        "\n",
        "# History object is a dictionary with keys.\n",
        "hd = history.history\n",
        "print(hd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnb-AnJQxm4x"
      },
      "outputs": [],
      "source": [
        "loss_tr = hd['loss']\n",
        "loss_va = hd['val_loss']\n",
        "epochs = range(0, ne)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "plt.plot(epochs, loss_tr, '-.o', label='Training loss')\n",
        "plt.plot(epochs, loss_va, 'r', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjVk1X5uxm4x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRhdTnzNxm4x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph6c0SLbxm4y"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "- May help against overfitting\n",
        "- by improving generalization\n",
        "\n",
        "``The Dropout layer randomly sets input units to 0 with a frequency of `rate` at each step during training time,..\"\n",
        "https://keras.io/api/layers/regularization_layers/dropout/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZs3PGQlxm4y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "#Old Syntax:\n",
        "# from keras import layers\n",
        "\n",
        "#Define your NN arch\n",
        "def myNN(train_data):\n",
        "#Initiate model:\n",
        "    model = models.Sequential()\n",
        "#First layer is special: It needs to match the shape of data:\n",
        "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],), name='in'))\n",
        "    model.add(layers.Dropout(0.15))\n",
        "    # units = 64: # of neurons; determines output shape of the layer.\n",
        "    # input_shape: how many W needed per neuron?\n",
        "    # Thus: input arrays of shape (*, 13). Output arrays of shape: (*, 64)\n",
        "#Hidden layers will auto-size their input based the previous layer:\n",
        "    model.add(layers.Dense(64, activation='relu', name='hidden1'))\n",
        "    #Can add more layers here to make it deeper...\n",
        "    model.add(layers.Dropout(0.15))\n",
        "\n",
        "#Output: Need a single number as our prediction:\n",
        "    model.add(layers.Dense(1, name='pred'))\n",
        "\n",
        "    #Note that output layer has no activation\n",
        "    # b/c this is a regression problem, not classification.\n",
        "#How should the weights be calculated?\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics = ['mae'])\n",
        "    return model\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIWVqco1xm4y"
      },
      "outputs": [],
      "source": [
        "#build the model\n",
        "model = myNN(Xtrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZYU4kEpxm4y"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5RqmAkuxm4y"
      },
      "source": [
        "## Train/Test your model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZPfp60Wxm4y"
      },
      "outputs": [],
      "source": [
        "#Train: a la scikit:\n",
        "#Scikit: model.fit(Xtrain, ytrain)\n",
        "ne = 10\n",
        "model.fit(Xtrain, ytrain, epochs=ne, verbose=1) #, batch_size=1\n",
        "\n",
        "#Evalute our model:\n",
        "nnmse, nnmae = model.evaluate(Xtest, ytest, verbose = 1)\n",
        "print('NN Test MAE: ', nnmae)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- Design our fist neural net to solve a regression problem\n",
        "- Introduce tensorflow's Keras library\n",
        "- Evaluating trained model\n",
        "- Visualizing the loss\n",
        "- Improvements:\n",
        "  - Hidden layers\n",
        "  - Pre-processing: Scaling\n",
        "  - Dropout\n"
      ],
      "metadata": {
        "id": "mDk698WU6D85"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}